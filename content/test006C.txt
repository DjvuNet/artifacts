/*** Page =0= ***/
English Conversational Telephone Speech Recognition by Humans and 
Machines 
George Saon, Gakuto Kurata∗ , Tom Sercu 
Kartik Audhkhasi, Samuel Thomas, Dimitrios Dimitriadis 
Xiaodong Cui, Bhuvana Ramabhadran, Michael Picheny 
IBM Watson, Yorktown Heights, USA 
∗IBM Research - Tokyo, Japan 
Lynn-Li Lim, Bergul Roomi, Phil Hall 
Appen, Sydney, Australia 
Abstract 
One of the most difﬁcult speech recognition tasks is accu- 
rate recognition of human to human communication. Advances 
in deep learning over the last few years have produced major 
speech recognition improvements on the representative Switch- 
board conversational corpus. Word error rates that just a few 
years ago were 14% have dropped to 8.0%, then 6.6% and most 
recently 5.8%, and are now believed to be within striking range 
of human performance. This then raises two issues - what IS hu- 
man performance, and how far down can we still drive speech 
recognition error rates? A recent paper by Microsoft suggests 
that we have already achieved human performance. In trying 
to verify this statement, we performed an independent set of 
human performance measurements on two conversational tasks 
and found that human performance may be considerably better 
than what was earlier reported, giving the community a signiﬁ- 
cantly harder goal to achieve. We also report on our own efforts 
in this area, presenting a set of acoustic and language model- 
ing techniques that lowered the word error rate of our own En- 
glish conversational telephone LVCSR system to the level of 
5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 
2000 evaluation, which - at least at the writing of this paper - is 
a new performance milestone (albeit not at what we measure 
to be human performance!). On the acoustic side, we use a 
score fusion of three models: one LSTM with multiple feature 
inputs, a second LSTM trained with speaker-adversarial multi- 
task learning and a third residual net (ResNet) with 25 convo- 
lutional layers and time-dilated convolutions. On the language 
modeling side, we use word and character LSTMs and convo- 
lutional WaveNet-style language models. 
Index Terms: LSTM, ResNet, dilated convolutions, conversa- 
tional speech recognition 
1. Introduction 
With the performance of ASR systems inching ever closer to 
that of humans, it is important to benchmark human perfor- 
mance accurately. In [1], the authors claim a human word er- 
ror rate (WER) of 5.9%/11.3% on the Switchboard/CallHome 
subsets (SWB/CH) of the NIST Hub5 2000 evaluation testset. 
When compared with [2] which quotes a WER of 4%, the 5.9% 
estimate seemed rather high (albeit measured on different data). 
This intriguing discrepancy prompted us to launch our own hu- 
man transcription effort in order to conﬁrm (or disconﬁrm) the 
estimates from [1]. The ﬁndings from this effort were doubly 
surprising. First, we were expecting the SWB measurement 
to be closer to the Lippmann estimate of 4% but could only 
get down to 5.1% for the best transcriber after quality checks. 
Second, the same transcriber achieved a surprisingly low 6.8% 
WER for CallHome (we were expecting a much higher number 
based on the 11.3% estimate). 
For comparison, our latest ASR system achieves 
5.5%/10.3% WER on SWB/CH. This means that “human 
parity” is attainable on this particular Switchboard subset 
(although not achieved yet) but is a distant dream for the 
CallHome task. What makes the Switchboard and CallHome 
testsets so different one might ask? The biggest problem with 
the SWB testset is that 36 out of 40 test speakers appear in the 
training data, some in as many as 8 different conversations [3], 
and our acoustic models are very good at memorizing speech 
patterns seen during training. The second problem is that the 
SWB and CH tasks differ in the style of conversational speech: 
SWB consists of conversations between strangers while CH 
consists of calls between friends and family members. Speak- 
ing style between strangers tends to be more formal whereas 
the CallHome style is more casual making CH a harder task. 
The training data collected by LDC under the Switchboard and 
Fisher protocols is almost entirely Switchboard-like meaning 
that testing on CallHome is a mismatched scenario for ASR 
systems. Since ASR systems are generally not robust to 
mismatched training and testing conditions, it comes as no 
surprise that the degradation in performance from SWB to CH 
for ASR systems is larger than that of expert transcribers. 
On the system side, we have simpliﬁed and improved our 
acoustic models considerably and experimented with more so- 
phisticated language models such as LSTM and WaveNet LMs. 
Most of the AM improvement comes from LSTMs that operate 
on multiple features or use a different training criterion such as 
speaker-adversarial multi-task learning. Additionally, replacing 
the VGG convolutional nets [4] that we had in our last year’s 
system [5] with ResNets [6] turned out to be beneﬁcial for per- 
formance. On the LM side, adding LSTM word and character- 
based LMs resulted in substantial accuracy gains. 
The rest of the paper is organized as follows: in section 2 
we talk about the human transcription experiments; in section 3 
we describe a series of system improvements pertaining to both 
acoustic and language modeling and in section 4 we summarize 
our ﬁndings. 
2. Human transcription experiments 
These experiments were carried out by IBM’s preferred speech 
transcription provider, Appen. The transcription protocol that 
was agreed upon was to have three independent transcribers 
provide transcripts which were quality checked by a fourth se- 
nior transcriber. All four transcribers are native US English 
arXiv:1703.02136v1 [cs.CL] 6 Mar 2017

/*** End Page =0= ***/
/*** Page =1= ***/
speakers and were selected based on the quality of their work 
on past transcription projects. The transcribers were familiar- 
ized with the LDC transcription guidelines which cover hyphen- 
ations, spelled abbreviations, contractions, partial words, non- 
speech sounds, etc. 
The transcription time was estimated at 12-14 times real- 
time (xRT) for the ﬁrst pass for Transcribers 1-3 and an addi- 
tional 1.7-2xRT for the second quality checking pass (by Tran- 
scriber 4). Both passes involved listening to the audio multiple 
times: around 3-4 times for the ﬁrst pass and 1-2 times for the 
second. After receiving the transcripts, the following ﬁltering 
rules were aplied: 
• All non-speech markers were tagged as non-lexical items 
which are ignored during scoring. Examples of non- 
speech markers are: [laughter], [breathing], [noise], {no 
speech}, etc. 
• Other markers such as ’...’, ’–’, ’(( ))’ were eliminated 
prior to scoring. 
• All partial words ending in ’-’ were marked as non- 
lexical items. 
• All punctuation marks such as ’.’, ’,’, ’!’ and ’?’ were 
eliminated prior to scoring. 
In order to use NIST’s scoring tool sclite, we had to 
convert the transcripts into CTM ﬁles which have time-marked 
word boundary information. This was done by splitting the du- 
ration of the utterance uniformly across the number of words. 
In Table 1 we show the error rates of the three transcribers 
before and after quality checking by the fourth transcriber as 
well as the human WER reported in [1]. Unsurprisingly, there is 
some variation among transcriber performance and the quality 
checking pass reduces the error rate across all transcribers. 
WER SWB WER CH 
Transcriber 1 raw 
6.1 
8.7 
Transcriber 1 QC 
5.6 
7.8 
Transcriber 2 raw 
5.3 
6.9 
Transcriber 2 QC 
5.1 
6.8 
Transcriber 3 raw 
5.7 
8.0 
Transcriber 3 QC 
5.2 
7.6 
Human WER from [1] 5.9 
11.3 
Table 1: Word error rates on SWB and CH for human tran- 
scribers before and after quality checking contrasted with the 
human WER reported in [1]. 
Additionally, in Tables 2 and 3, we take a closer look at 
the most frequent substitution, deletion and insertion errors for 
our system output and the best human transcript after quality 
checking. While many of the errors look similar to those re- 
ported in [1], there is a glaring discrepancy in the frequency 
of top deletions for CallHome between our human transcript 
and theirs. This suggests that the very different estimates for 
the human error rate for CallHome (6.8% versus 11.3%) can 
be attributed to a much lower deletion rate for our best human 
transcript. 
3. System improvements 
In this section we discuss the training data and testsets that were 
used as well as improvements in acoustic and language model- 
ing. The training set for our acoustic models consists of 262 
SWB 
CH 
ASR 
Human 
ASR 
Human 
11:and/in 
16:(%hes)/oh 21: was/is 
28: (%hes) / oh 
9:was/is 
12:was/is 
16:him/them 22:was/is 
7:it/that 
7: (i-) / %hes 
15: in / and 11: (%hes) / %bcack 
6: (%hes) / oh 
5: (%hes) / a 
8:a/the 
10: bentsy / benji 
6: him / them 
5:(%hes)/hmm 8: and/in 
10: yeah / yep 
6:too/to 
5:(a-)/%hes 8:is/was 
9:a/the 
5: (%hes) / i 
5:could/can 8:two/to 
8:is/was 
5: then / and 
5:that/it 
7:the/a 
7: (%hes) / a 
4: (%hes) / %bcack 4: %bcack / oh 7: too / to 
7:the/a 
4: (%hes) / am 
4:and/in 
6:(%hes)/a 7: well/oh 
Table 2: Most frequent substitution errors for humans and ASR 
system on SWB and CH. 
Deletions 
Insertions 
SWB 
CH 
SWB 
CH 
ASR 
Human ASR 
Human ASR Human ASR Human 
30: it 
19:i 46:i 
20: i 
13:i 16:is 
23:a 17:is 
20: i 
17: it 46: it 
18:and 10:a 14:%hes 14:is 17:it 
17: that 
16: and 39: and 
15: it 
7:and 12:i 
11: i 16: and 
16: a 
14: that 32: is 
15: the 7: of 11: and 10: are 14: have 
14: and 
14: you 26: oh 
14:is 6:you 9:it 
10: you 13: a 
14: oh 
12:is 25:a 
13:not 5:do 6:do 
9: the 13: that 
14: you 
12: the 20: to 
10:a 5:the 5:have 8:have 12:i 
12: %bcack 11: a 19: that 
10: in 5: yeah 5: yeah 8: that 11: %hes 
12: the 
10: of 19: the 
10: that 4: air 5: you 7: and 10: not 
11: to 
9: have 18: %bcack 10: to 4: in 4: are 
7: it 
9: oh 
Table 3: Most frequent deletion and insertion errors for humans 
and ASR system on SWB and CH. 
hours of Switchboard 1 audio with transcripts provided by Mis- 
sissippi State University, 1698 hours from the Fisher data col- 
lection and 15 hours of CallHome audio. In order to allay fears 
that we may be overﬁtting to the Hub5 2000 testsets by exten- 
sively testing on them, we have decided to report results on a va- 
riety of testsets. Since the RT’02, RT’03, RT’04 and DEV’04f 
testsets have not been used in more than a decade, we are fairly 
conﬁdent that performance improvements on these testsets are 
indicative of real progress. Statistics about all the testsets used 
in the experiments are given in Table 4. 
Testset 
Duration Nb. speakers Nb. words 
Hub5’00 SWB 2.1h 
40 
21.4K 
Hub5’00 CH 
1.6h 
40 
21.6K 
RT’02 
6.4h 
120 
64.0K 
RT’03 
7.2h 
144 
76.0K 
RT’04 
3.4h 
72 
36.7K 
DEV’04f 
3.2h 
72 
37.8K 
Table 4: Testsets that are used to report experimental results. 
In [7], we have shown that convolutional and non- 
convolutional AMs have comparable performance and good 
complementarity. Hence, the strategy for our previous sys- 
tems [8, 5] was to use a combination of recurrent and convolu- 
tional nets. For example, in last year’s system we used a score 
fusion of three models which share the same decision tree: un- 
folded RNNs with maxout activations, LSTMs and VGG nets. 
This year, in order to simplify and enhance the overall architec- 
ture, we eliminated the maxout RNN, we improved the LSTMs 
and we replaced the VGG nets with residual nets (ResNets). 
3.1. LSTM acoustic models 
All models presented here share the following characteris- 
tics. Their architecture consists in 4-6 bidirectional layers 
with 1024 cells per layer (512 per direction), one linear bot- 

/*** End Page =1= ***/
/*** Page =2= ***/
tleneck layer with 256 units and an output layer with 32K 
units corresponding to as many context-dependent HMM states 
(shown on the left side of Figure 1). Training is done on non- 
overlapping subsequences of 21 frames where each frame con- 
sists of 40-dimensional FMLLR features to which we append 
100-dimensional i-vectors. We group subsequences from dif- 
ferent utterances into minibatches of size 128 for processing 
speed and reliable gradient estimates. The training consists 
of 14 passes of cross-entropy followed by 1 pass of SGD se- 
quence training using the boosted MMI criterion [9] smoothed 
by adding the scaled gradient of the cross-entropy loss [10]. Im- 
plementation of the LSTM was done in Torch [11] with cuDNN 
v5.0 backend. Cross-entropy training for each model took about 
2 weeks for 700M samples/epoch, on a single Nvidia K80 GPU 
device. 
The ﬁrst two improvements are fairly banal and consist in 
increasing the number of layers from 4 (like in our previous 
model [5]) to 6 and in realigning the training data with a 6-layer 
LSTM and retraining another LSTM. The effect of these steps 
is shown in the ﬁrst three rows of Table 5 across all testsets. 
LSTM 
SWB CH RT’02 RT’03 RT’04 DEV’04f 
4-layer 
8.0 14.3 12.2 11.6 11.0 
10.8 
6-layer 
7.7 14.0 11.8 11.4 10.8 
10.4 
Realigned 7.7 13.8 11.7 11.2 10.8 
10.2 
SA-MTL 
7.6 13.6 11.5 11.0 10.7 
10.1 
Feat. fusion 7.2 12.7 10.7 10.2 10.1 
9.6 
Table 5: Word error rates for LSTM AMs across all testsets 
(36M n-gram LM). 
The second set of experiments was centered around 
the use of speaker-adversarial multi-task learning (SA-MTL). 
In [12], the authors introduce domain-adversarial neural net- 
works which are models that are trained to not distiguish be- 
tween in-domain, labeled data and out-of-domain, unlabeled 
data. This is achieved by training a domain classiﬁer in parallel 
with the main classiﬁer and by subtracting the gradient compo- 
nent from the domain classiﬁer when estimating the parameters 
of the main classiﬁer. This idea has been successfully applied 
in speech by [13] in the context of noise robustness where the 
author proposes noise-adversarial MTL to suppress the effects 
of noise. Here, we experiment with training a speaker classi- 
ﬁer in addition to the main CD-HMM state classiﬁer in order to 
suppress the effects of speaker variability on ASR performance. 
Since i-vectors are a good low-dimensional representation of a 
speaker, we decided to train the speaker classiﬁer to predict the 
i-vector inputs using an MSE loss function. The speaker classi- 
ﬁer has one sigmoid layer and one hyperbolic tangent layer as 
shown in Figure 1. 
If we denote by θ, θc , θs the parameters of the common 
LSTM, the main classiﬁer (weights of linear layer before soft- 
max) and the speaker classiﬁer, the SGD update is done accord- 
ing to: 
ˆ 
θc 
= θc − ǫ∂LCE(x) 
∂θc 
ˆ 
θs 
= θs − ǫ∂LMSE(x) 
∂θs 
ˆ 
θ = θ−ǫ ∂LCE(x) 
∂θ −λ∂LMSE(x) 
∂θ 
where x denotes a minibatch, LCE , LM SE denote respectively 
256 linear 
32000 softmax 
1024 LSTM 
FMLLR i−vector 
1024 LSTM 
... 
1024 sigmoid 
100 tanh 
θ 
θs 
θc 
Figure 1: LSTM with speaker-adversarial MTL architecture. 
the cross-entropy loss of the main classiﬁer and the mean- 
squared error loss of the i-vector classiﬁer, λ is a scaling pa- 
rameter (typically set to 0.1), and ǫ is the learning rate. After 
the model is trained, the i-vector classiﬁer branch is discarded 
at test time. As can be seen from Table 5 rows 3 and 4, we ob- 
serve some small gains across all testsets which are also due in 
part to reestimating the VTLN warp factors and FMLLR trans- 
forms using an LSTM decoding output (old factors and trans- 
forms were based off a GMM decoding). 
Last but not least, the largest improvement in LSTM mod- 
eling was achieved through feature fusion. The thought pro- 
cess leading to this experiment was that we wanted to add 
utterance-level information to our models which were only 
looking at a window of 21 consecutive frames. One possi- 
bility was to train an end-to-end LSTM using CTC as in [14, 
15, 16, 17] and append the features from the last LSTM layer 
before the softmax to our existing features. This experiment 
worked quite well however, upon closer inspection, it turned 
out that the CTC model used a different set of input features: 
Logmel+∆+∆∆ instead of PLP followed by LDA and FM- 
LLR. The question then naturally arose whether the gains came 
from CTC modeling or from the different input representa- 
tions. To answer this question, we built an LSTM trained on 
fused FMLLR+i-vector+Logmel+∆+∆∆ features the standard 
way (without speaker-adversarial MTL). The WER improve- 
ment from adding the Logmel features, indicated in Table 5 
rows 3 and 5, is the same as with CTC features meaning that 
the CTC modeling step was not needed. Finally, we note that 
the feature fusion LSTM compares favorably with other single 
acoustic models from the literature as mentioned in [17] (Table 
4). 
3.2. ResNet acoustic models 
On the convolutional network acoustic modeling side, we 
trained residual networks with pre-activation identity shortcut 
connections. Residual Networks were introduced for image 
recognition in [18] and used in speech recognition in [1, 19]. 
The novelty of residual networks is to introduce shortcut con- 
nections between so-called “blocks” of convolutional layers, 
which was argued to improve the ﬂow of information and gra- 
dients, and allows training even deeper CNNs without the opti- 
mization problem occuring without the residual connections. 
Table 6 shows four residual network model architectures 

/*** End Page =2= ***/
/*** Page =3= ***/

/*** End Page =3= ***/
/*** Page =4= ***/
3.3. Model combination 
In Table 7 we report the performance of the best individual mod- 
els described in the previous paragraphs as well as the results 
after frame-level score fusion across all testsets. All decodings 
are done with an 85K word vocabulary and a 4-gram language 
model with 36M n-grams. We note that LSTMs and ResNets 
exhibit a strong complementarity which improves the WER for 
all testsets. 
Model 
SWB CH RT’02 RT’03 RT’04 DEV’04f 
LSTM1 (SA-MTL) 
7.6 13.6 11.5 11.0 10.7 
10.1 
LSTM2 (Feat. fusion) 7.2 12.7 10.7 10.2 10.1 
9.6 
ResNet 
7.6 14.5 12.2 12.2 11.5 11.1 
ResNet+LSTM2 
6.8 12.2 10.2 10.0 9.7 
9.4 
ResNet+LSTM1+LSTM2 6.7 12.1 10.1 10.0 9.7 
9.2 
Table 7: Word error rates for LSTMs and ResNet and frame- 
level score fusion results across all testsets (36M n-gram LM). 
3.4. Language modeling improvements 
In addition to n-gram and model-M used in our previous sys- 
tem [5], we introduced LSTM-based as well as convolution- 
based LMs in this paper. 
We experimented with four LSTM LMs, namely Word- 
LSTM, Char-LSTM, Word-LSTM-MTL, and Char-LSTM-MTL. 
The Word-LSTM had one word-embeddings layer, two LSTM 
layers, one fully-connected layer, and one softmax layer, as 
shown in Figure 3. The upper LSTM layer and the fully- 
connected layer were wrapped by residual connections [6]. 
Dropout was only applied to the vertical dimension and not ap- 
plied to the time dimension [25]. The Char-LSTM added an ad- 
ditional LSTM layer to estimate word-embeddings from charac- 
ter sequences as illustrated in Figure 4 [26]. Both Word-LSTM 
and Char-LSTM used the cross-entropy loss of predicting the 
next word given its history as objective function, similar to con- 
ventional LMs. In addition, we introduced multi-task learning 
(MTL) in Word-LSTM-MTL and Char-LSTM-MTL. We ﬁrst 
clustered the vocabulary using Brown clustering [27]. When 
training Word-LSTM-MTL and Char-LSTM-MTL, weighted 
summation of cross-entropy of predicting next word given its 
history and next class given its history was used as objective 
function. 
Inspired by the complementarity of convolutional and 
non-convolutional acoustic models, we experimented with a 
convolution-based LM in the form of dilated causal convolution 
as used in WAVENET [28]. The resulting model is called Word- 
DCC and consists of word-embeddings layer, causal convolu- 
tion layers with dilation, convolution layers, fully-connected 
layers, softmax layer, and residual connections. The actual 
number of layers and dilation/window sizes were determined 
using heldout data (Figure 5 has a simple conﬁguration for il- 
lustration purposes). 
For these ﬁve LMs, the training data and training proce- 
dures are common and described below: 
• We used the same vocabulary of 85K words from [5]. 
• We ﬁrst train the LM with a corpus of 560M words 
consisting of publicly available text data from LDC, in- 
cluding Switchboard, Fisher, Gigaword, and Brodcast 
News and Conversations. Then, starting from the trained 
model, we further train the LM with only the transcripts 
of the 1975 hours audio data used to train the acoustic 
model, consisting of 24M words. 
WER [%] 
SWB CH 
n-gram 
6.7 12.1 
n-gram + model-M 
6.1 11.2 
n-gram + model-M + Word-LSTM 
5.6 10.4 
n-gram + model-M + Char-LSTM 
5.7 10.6 
n-gram + model-M + Word-LSTM-MTL 5.6 10.3 
n-gram + model-M + Char-LSTM-MTL 
5.6 10.4 
n-gram + model-M + Word-DCC 
5.8 10.8 
n-gram + model-M + 4 LSTMs + DCC 
5.5 10.3 
Table 8: WER on SWB and CH with various LM conﬁgurations. 
• We controlled the leaning rate by ADAM [29] and intro- 
duced a self-stabilization term to coordinate the layer- 
wise learning rates [30]. 
• For all models, we tuned the hyper-parameters based on 
the perplexity of the heldout data which is a subset of the 
acoustic transcripts. The approximate number of param- 
eters for each model was 90M to 130M. 
We ﬁrst generated word lattices with the n-gram LM and 
our best acoustic model consisting of ResNet and two LSTMs. 
Then we rescored the word lattices with the model-M and gen- 
erated n-best lists from the rescored lattice. Finally, we applied 
the four LSTM-based LMs and the convolution-based LM. Note 
that LM probabilities were linearly interpolated and the interpo- 
lation weights of all LMs were estimated using the heldout data. 
Table 8 shows WER on SWB and CH with various LM 
conﬁgurations. The LSTM-based LMs show signiﬁcant im- 
provements over the strong n-gram + model-M results. The 
Word-DCC also has a marginal improvement over the n-gram 
+ model-M. The effect of multi-task learning was conﬁrmed es- 
pecially on CH. Among the ﬁve LSTM-based and convolution- 
based LMs, word-LSTM-MTL achieved the best WER of 5.6% 
and 10.3% on SWB and CH respectively. By combining ﬁve 
LMs on top of n-gram + model-M, we achieved 5.5% and 
10.3% WER for SWB and CH respectively. Lastly, we sum- 
marize the improvements due to the various language model 
rescoring steps across all testsets in Table 9. We noticed that 
the testset references have inconsistent transcription conven- 
tions with regards to spellings which are not followed by pe- 
riods for SWB and CH (e.g. T V) and followed by periods for 
the other testsets (such as T. V.). The last line of Table 9 shows 
the WERs when periods are removed from both the references 
and system outputs by adding the ﬁltering rules A. => A ... Z. 
=>ZtotheGLMﬁle. 
More details about the language modeling are given in a 
companion paper [31]. 
SWB CH RT’02 RT’03 RT’04 DEV’04f 
n-gram 
6.7 12.1 10.1 10.0 9.7 
9.2 
+ model-M 
6.1 11.2 9.4 9.4 9.0 
8.8 
+ LSTM + DCC 5.5 10.3 8.3 8.3 8.0 
8.0 
’.’ removal 
5.5 10.3 8.3 8.0 7.7 
7.1 
Table 9: Word error rates for the different LM rescoring steps 
across all testsets. Last line shows WERs after ’.’ removal from 
the references and system outputs. 

/*** End Page =4= ***/
/*** Page =5= ***/

/*** End Page =5= ***/
/*** Page =6= ***/
[16] H. Soltau, H. Liao, and H. Sak, “Neural speech recog- 
nizer: Acoustic-to-word lstm model for large vocabulary 
speech recognition,” arXiv preprint arXiv:1610.09975, 
2016. 
[17] L. Hairong, Z. Zhu, X. Li, and S. Satheesh, “Gram-ctc: 
Automatic unit selection and target decomposition for se- 
quence labelling,” CoRR arXiv:1703.00096, 2017. 
[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual 
learning for image recognition,” CoRR arXiv:1512.03385, 
2015. 
[19] Y. Zhang, W. Chan, and N. Jaitly, “Very deep convolu- 
tional networks for end-to-end speech recognition,” Proc. 
ICASSP, 2017. 
[20] S. Zagoruyko and N. Komodakis, “Wide residual net- 
works,” arXiv preprint arXiv:1605.07146, 2016. 
[21] K. He, X. Zhang, S. Ren, and J. Sun, “Identity 
mappings in deep residual networks,” arXiv preprint 
arXiv:1603.05027, 2016. 
[22] T. Sercu, C. Puhrsch, B. Kingsbury, and Y. LeCun, 
“Very deep multilingual convolutional neural networks for 
lvcsr,” Proc. ICASSP, 2016. 
[23] T. Sercu and V. Goel, “Advances in very deep convolu- 
tional neural networks for lvcsr,” arXiv, 2016. 
[24] ——, “Dense prediction on sequences with time-dilated 
convolutions for speech recognition,” NIPS End-to-end 
Learning for Speech and Audio Processing Workshop, 
2016. 
[25] W. Zaremba, I. Sutskever, and O. Vinyals, “Re- 
current neural network regularization,” arXiv preprint 
arXiv:1409.2329, 2014. 
[26] W. Ling, T. Lu´ıs, L. Marujo, R. F. Astudillo, S. Amir, 
C. Dyer, A. W. Black, and I. Trancoso, “Finding 
function in form: Compositional character models for 
open vocabulary word representation,” arXiv preprint 
arXiv:1508.02096, 2015. 
[27] P. F. Brown, P. V. Desouza, R. L. Mercer, V. J. D. Pietra, 
and J. C. Lai, “Class-based n-gram models of natural lan- 
guage,” Computational Linguistics, vol. 18, no. 4, pp. 
467–479, 1992. 
[28] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, 
O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and 
K. Kavukcuoglu, “WaveNet: A generative model for raw 
audio,” arXiv preprint arXiv:1609.03499, 2016. 
[29] D. Kingma and J. Ba, “ADAM: A method for stochastic 
optimization,” arXiv preprint arXiv:1412.6980, 2014. 
[30] P. Ghahremani and J. Droppo, “Self-stabilized deep neural 
network,” in Proc. ICASSP, 2016, pp. 6645–6649. 
[31] G. Kurata, A. Sethy, B. Ramabhadran, and G. Saon, “Em- 
pirical exploration of LSTM and CNN language models 
for speech recognition,” Submitted to Interspeech 2017, 
2017. 

/*** End Page =6= ***/
